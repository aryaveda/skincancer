{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":20270,"databundleVersionId":1222630,"sourceType":"competition"},{"sourceId":1339703,"sourceType":"datasetVersion","datasetId":756377},{"sourceId":1353814,"sourceType":"datasetVersion","datasetId":762233},{"sourceId":1444814,"sourceType":"datasetVersion","datasetId":846815}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1st Place Solution Best Model Inference Code\n\nHi, all\n\nSome of our friends want to try our trained models on their own moles ;)\n\nSo we decide to publish this kernel.\n\nThis is our infernce code for one of our best single model (Effnet-B7 w/ input size 640),\nwhich have a cv auc_all around `0.975` (validation on both 18,19,20) using chris's splits.\n\n(To see this auc_all score you only need to set `DEBUG = False` then rerun this kernel, it takes around 7h~ to compute the whole oof on kaggle kernel)\n\n\n# Usage\n\nOne can use this kernel to check your own moles by:\n\n* Take some pictures on your moles by your phone or camera.\n* Upload it to Kaggle Datasets (remember to make it private)\n* Fork this kernel\n* Add your uploaded dataset\n* Modify `Predict` Section to predict on your own pictures!\n\n# Thanks","metadata":{}},{"cell_type":"code","source":"DEBUG = True","metadata":{"execution":{"iopub.status.busy":"2025-02-13T07:24:43.285627Z","iopub.execute_input":"2025-02-13T07:24:43.286009Z","iopub.status.idle":"2025-02-13T07:24:43.290056Z","shell.execute_reply.started":"2025-02-13T07:24:43.285980Z","shell.execute_reply":"2025-02-13T07:24:43.288988Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip -q install geffnet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-02-13T07:24:43.291265Z","iopub.execute_input":"2025-02-13T07:24:43.291596Z","iopub.status.idle":"2025-02-13T07:24:47.669197Z","shell.execute_reply.started":"2025-02-13T07:24:43.291563Z","shell.execute_reply":"2025-02-13T07:24:47.668415Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport sys\nimport time\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport PIL.Image\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, auc\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport albumentations as A\nimport geffnet\n\ndevice = torch.device('cuda')","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2025-02-13T07:24:47.671359Z","iopub.execute_input":"2025-02-13T07:24:47.671610Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.4 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"kernel_type = '9c_b7ns_1e_640_ext_15ep'\nimage_size = 640\nuse_amp = False\ndata_dir = '../input/jpeg-melanoma-768x768'\ndata_dir2 = '../input/jpeg-isic2019-768x768'\nmodel_dir = '../input/melanoma-winning-models'\nenet_type = 'efficientnet-b7'\nbatch_size = 16\nnum_workers = 4\nout_dim = 9\n\nuse_meta = False\nuse_external = '_ext' in kernel_type","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Read CSV","metadata":{}},{"cell_type":"code","source":"# Read the test data CSV file from the directory where your data lives. \n# Basically, we're grabbing the cheat sheet for the test images.\ndf_test = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n\n# Now, we're adding a new column called 'filepath' to the dataframe. \n# This shit is gonna store the full path to each test image.\n# We're using the 'image_name' column and saying, \"Hey, for every image name, \n# slap it together with the data directory and the 'test' folder, and don't forget the '.jpg' extension, you dumbass.\"\ndf_test['filepath'] = df_test['image_name'].apply(lambda x: os.path.join(data_dir, 'test', f'{x}.jpg'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load training data\nprint(\"Loading training dataset...\")\ndf_train = pd.read_csv(os.path.join(data_dir, 'train.csv'))\nprint(f\"Original df_train shape: {df_train.shape}\")\nprint(df_train['tfrecord'].unique())  # Shows all unique tfrecord values\nprint(len(df_train['tfrecord'].unique()))  # Counts unique tfrecord values\n# Filtering out records where tfrecord == -1\ndf_train = df_train[df_train['tfrecord'] != -1].reset_index(drop=True)\nprint(f\"After filtering tfrecord != -1, new shape: {df_train.shape}\")\n\n# Mapping tfrecord to fold\ntfrecord2fold = {\n    2:0, 4:0, 5:0,   # tfrecord 2, 4, 5 → fold 0\n    1:1, 10:1, 13:1, # tfrecord 1, 10, 13 → fold 1\n    0:2, 9:2, 12:2,  # tfrecord 0, 9, 12 → fold 2\n    3:3, 8:3, 11:3,  # tfrecord 3, 8, 11 → fold 3\n    6:4, 7:4, 14:4,  # tfrecord 6, 7, 14 → fold 4\n}\ndf_train['fold'] = df_train['tfrecord'].map(tfrecord2fold)\nprint(\"Added 'fold' column based on tfrecord mapping.\")\nprint(df_train[['tfrecord', 'fold']].head())\n\n# Marking dataset as internal (is_ext = 0)\ndf_train['is_ext'] = 0\nprint(\"Added 'is_ext' column with value 0 for internal dataset.\")\n\n# Creating full file paths for images\ndf_train['filepath'] = df_train['image_name'].apply(lambda x: os.path.join(data_dir, 'train', f'{x}.jpg'))\nprint(\"Added 'filepath' column. Sample paths:\")\nprint(df_train['filepath'].head())\n\n# Normalizing diagnosis labels\nprint(\"Before diagnosis standardization:\\n\", df_train['diagnosis'].value_counts())\n\ndf_train['diagnosis'] = df_train['diagnosis'].replace({\n    'seborrheic keratosis': 'BKL',\n    'lichenoid keratosis': 'BKL',\n    'solar lentigo': 'BKL',\n    'lentigo NOS': 'BKL',\n    'cafe-au-lait macule': 'unknown',\n    'atypical melanocytic proliferation': 'unknown'\n})\n\nprint(\"After diagnosis standardization:\\n\", df_train['diagnosis'].value_counts())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If use_external is True, we load and clean some external training data.\n\nWe assign folds, add file paths, and clean up diagnosis labels in the external data.\n\nWe combine the original training data with the external data into one big happy dataframe.\n\nWe create a mapping from diagnosis labels to numerical indices (because machines are bad at words).\n\nWe grab the index for 'melanoma' because it’s probably important.\n\nFinally, we print out the diagnosis-to-index mapping to see what’s what.","metadata":{}},{"cell_type":"code","source":"# Load external dataset if use_external is enabled\nif use_external:\n    print(\"Loading external dataset...\")\n    df_train2 = pd.read_csv(os.path.join(data_dir2, 'train.csv'))\n    print(f\"Original external dataset shape: {df_train2.shape}\")\n\n    df_train2 = df_train2[df_train2['tfrecord'] >= 0].reset_index(drop=True)\n    print(f\"After filtering tfrecord >= 0, new shape: {df_train2.shape}\")\n\n    df_train2['fold'] = df_train2['tfrecord'] % 5\n    df_train2['is_ext'] = 1\n    df_train2['filepath'] = df_train2['image_name'].apply(lambda x: os.path.join(data_dir2, 'train', f'{x}.jpg'))\n\n    df_train2['diagnosis'] = df_train2['diagnosis'].replace({\n        'NV': 'nevus',\n        'MEL': 'melanoma'\n    })\n\n    print(\"After standardizing external dataset diagnoses:\\n\", df_train2['diagnosis'].value_counts())\n\n    # Merge external dataset with main dataset\n    df_train = pd.concat([df_train, df_train2]).reset_index(drop=True)\n    print(f\"After merging, final dataset shape: {df_train.shape}\")\n\n# Creating label-to-index mapping\ndiagnosis2idx = {d: idx for idx, d in enumerate(sorted(df_train.diagnosis.unique()))}\ndf_train['target'] = df_train['diagnosis'].map(diagnosis2idx)\n\nprint(\"Final diagnosis mapping (label → index):\")\nfor key, value in diagnosis2idx.items():\n    print(f\"{key}: {value}\")\n\nmel_idx = diagnosis2idx['melanoma']\nprint(f\"'melanoma' class index: {mel_idx}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class SIIMISICDataset(Dataset):\n    def __init__(self, csv, split, mode, transform=None):\n\n        self.csv = csv.reset_index(drop=True)\n        self.split = split\n        self.mode = mode\n        self.transform = transform\n\n    def __len__(self):\n        return self.csv.shape[0]\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n        \n        image = cv2.imread(row.filepath)\n        image = image[:, :, ::-1]\n\n        if self.transform is not None:\n            res = self.transform(image=image)\n            image = res['image'].astype(np.float32)\n        else:\n            image = image.astype(np.float32)\n\n        image = image.transpose(2, 0, 1)\n\n        if self.mode == 'test':\n            return torch.tensor(image).float()\n        else:\n            return torch.tensor(image).float(), torch.tensor(self.csv.iloc[index].target).long()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df_train['target'].unique())\n\n# Create reverse mapping from index to diagnosis\nidx2diagnosis = {v: k for k, v in diagnosis2idx.items()}  # Invert the dictionary\n\n# Select 30 random images from df_train\nsample_rows = df_train.sample(30)\n\n# Create a figure with a grid of 5x6\nfig, axes = plt.subplots(5, 6, figsize=(15, 12))\n\n# Iterate over each image and display it\nfor ax, (_, row) in zip(axes.flatten(), sample_rows.iterrows()):\n    image_path = row.filepath  # Get the image path\n    image = cv2.imread(image_path)  # Load the image\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n\n    diagnosis_label = idx2diagnosis[row.target]  # Convert target number to text label\n\n    ax.imshow(image)\n    ax.axis(\"off\")  # Hide axes\n    ax.set_title(f\"{diagnosis_label}\")  # Show diagnosis label instead of number\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transforms_val = A.Compose([\n    A.Resize(image_size, image_size),\n    A.Normalize()\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_show = df_train.sample(1000)\ndataset_show = SIIMISICDataset(df_show, 'train', 'val', transform=transforms_val)\n# dataset_show = CloudDataset(df_train, 'train', 'val', image_size, transform=None)\n# dataset_show = CloudDataset(df_test, 'test', 'test', image_size, transform=None)\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 20,10\nfor i in range(2):\n    f, axarr = plt.subplots(1,5)\n    for p in range(5):\n        idx = np.random.randint(0, len(dataset_show))\n        img, label = dataset_show[idx]\n        if use_meta:\n            img = img[0]\n        axarr[p].imshow(img.transpose(0, 1).transpose(1,2).squeeze())\n        axarr[p].set_title(str(label))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class enetv2(nn.Module):\n    def __init__(self, backbone, out_dim, n_meta_features=0, load_pretrained=False):\n\n        super(enetv2, self).__init__()\n        self.n_meta_features = n_meta_features\n        self.enet = geffnet.create_model(enet_type.replace('-', '_'), pretrained=load_pretrained)\n        self.dropout = nn.Dropout(0.5)\n\n        in_ch = self.enet.classifier.in_features\n        self.myfc = nn.Linear(in_ch, out_dim)\n        self.enet.classifier = nn.Identity()\n\n    def extract(self, x):\n        x = self.enet(x)\n        return x\n\n    def forward(self, x, x_meta=None):\n        x = self.extract(x).squeeze(-1).squeeze(-1)\n        x = self.myfc(self.dropout(x))\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test-Time Augmentation Function","metadata":{}},{"cell_type":"code","source":"def get_trans(img, I):\n    if I >= 4:\n        img = img.transpose(2,3)\n    if I % 4 == 0:\n        return img\n    elif I % 4 == 1:\n        return img.flip(2)\n    elif I % 4 == 2:\n        return img.flip(3)\n    elif I % 4 == 3:\n        return img.flip(2).flip(3)\n\n    \ndef val_epoch(model, loader, is_ext=None, n_test=1, get_output=False):\n    model.eval()\n    LOGITS = []\n    PROBS = []\n    TARGETS = []\n    with torch.no_grad():\n        for (data, target) in tqdm(loader):\n            \n            if use_meta:\n                data, meta = data\n                data, meta, target = data.to(device), meta.to(device), target.to(device)\n                logits = torch.zeros((data.shape[0], out_dim)).to(device)\n                probs = torch.zeros((data.shape[0], out_dim)).to(device)\n                for I in range(n_test):\n                    l = model(get_trans(data, I), meta)\n                    logits += l\n                    probs += l.softmax(1)\n            else:\n                data, target = data.to(device), target.to(device)\n                logits = torch.zeros((data.shape[0], out_dim)).to(device)\n                probs = torch.zeros((data.shape[0], out_dim)).to(device)\n                for I in range(n_test):\n                    l = model(get_trans(data, I))\n                    logits += l\n                    probs += l.softmax(1)\n            logits /= n_test\n            probs /= n_test\n\n            LOGITS.append(logits.detach().cpu())\n            PROBS.append(probs.detach().cpu())\n            TARGETS.append(target.detach().cpu())\n\n    LOGITS = torch.cat(LOGITS).numpy()\n    PROBS = torch.cat(PROBS).numpy()\n    TARGETS = torch.cat(TARGETS).numpy()\n\n    if get_output:\n        return LOGITS, PROBS\n    else:\n        acc = (PROBS.argmax(1) == TARGETS).mean() * 100.\n        auc = roc_auc_score((TARGETS==mel_idx).astype(float), LOGITS[:, mel_idx])\n        auc_20 = roc_auc_score((TARGETS[is_ext==0]==mel_idx).astype(float), LOGITS[is_ext==0, mel_idx])\n        return val_loss, acc, auc, auc_20","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Validation Function","metadata":{}},{"cell_type":"code","source":"PROBS = []\ndfs = []\n\nfor fold in range(5):\n    i_fold = fold\n\n    df_valid = df_train[df_train['fold'] == i_fold]\n    if DEBUG:\n        df_valid = pd.concat([\n            df_valid[df_valid['target'] == mel_idx].sample(10),\n            df_valid[df_valid['target'] != mel_idx].sample(10)\n        ])\n    print(df_valid.shape)\n\n    dataset_valid = SIIMISICDataset(df_valid, 'train', 'val', transform=transforms_val)\n    valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, num_workers=num_workers)\n\n    model = enetv2(enet_type, n_meta_features=0, out_dim=out_dim)\n    model = model.to(device)\n    model_file = os.path.join(model_dir, f'{kernel_type}_best_fold{i_fold}.pth')\n    state_dict = torch.load(model_file)\n    state_dict = {k.replace('module.', ''): state_dict[k] for k in state_dict.keys()}\n    model.load_state_dict(state_dict, strict=True)\n    model.eval()\n\n    this_LOGITS, this_PROBS = val_epoch(model, valid_loader, is_ext=df_valid['is_ext'].values, n_test=8, get_output=True)\n    PROBS.append(this_PROBS)\n    dfs.append(df_valid)\n    \ndfs = pd.concat(dfs).reset_index(drop=True)\ndfs['pred'] = np.concatenate(PROBS).squeeze()[:, mel_idx]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Raw auc_all\nroc_auc_score(dfs['target'] == mel_idx, dfs['pred'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Rank per fold auc_all\ndfs2 = dfs.copy()\nfor i in range(5):\n    dfs2.loc[dfs2['fold']==i, 'pred'] = dfs2.loc[dfs2['fold']==i, 'pred'].rank(pct=True)\nroc_auc_score(dfs2['target'] == mel_idx, dfs2['pred'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Raw auc_2020\nroc_auc_score(dfs[dfs['is_ext']==0]['target']==mel_idx, dfs[dfs['is_ext']==0]['pred'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Rank per fold auc_all\ndfs2 = dfs.copy()\nfor i in range(5):\n    dfs2.loc[dfs2['fold']==i, 'pred'] = dfs2.loc[dfs2['fold']==i, 'pred'].rank(pct=True)\nroc_auc_score(dfs2['target'] == mel_idx, dfs2['pred'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Rank per fold auc_2020\ndfs2 = dfs[dfs.is_ext==0].copy()\nfor i in range(5):\n    dfs2.loc[dfs2['fold']==i, 'pred'] = dfs2.loc[dfs2['fold']==i, 'pred'].rank(pct=True)\nroc_auc_score(dfs2['target'] == mel_idx, dfs2['pred'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade ipywidgets\n!jupyter nbextension enable --py widgetsnbextension","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Evaluation and Visualization (with detailed explanations and saving to CSV) ---\n\n# Create a directory to store results\nresults_dir = 'evaluation_results'\nos.makedirs(results_dir, exist_ok=True)\n\n# Define a unique filename for this run (based on model type and metadata usage)\nrun_id = f\"{enet_type}_metadata_{use_meta}\"\nresults_filename = os.path.join(results_dir, f\"{run_id}_results.csv\")\npredictions_filename = os.path.join(results_dir, f\"{run_id}_predictions.csv\")\n\n# 1. ROC AUC and ROC Curve\nroc_auc = roc_auc_score(dfs['target'] == mel_idx, dfs['pred'])\nprint(f'Raw auc_all: {roc_auc:.4f}')\n\nfpr, tpr, thresholds = roc_curve(dfs['target'] == mel_idx, dfs['pred'])\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.title(f'ROC Curve ({run_id})\\nAUC: {roc_auc:.4f}')  # Add run_id to title\nplt.legend(loc=\"lower right\")\nplt.text(0.6, 0.2, f\"AUC: {roc_auc:.4f}\", fontsize=12)  # Add AUC text to the plot\nplt.savefig(os.path.join(results_dir, f\"{run_id}_roc_curve.png\"))  # Save the plot\nplt.show()\n\nprint(\"\\n--- ROC Curve Explanation ---\")\nprint(\"The ROC curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.\")\nprint(\"TPR (Sensitivity) = True Positives / (True Positives + False Negatives)\")\nprint(\"FPR (1 - Specificity) = False Positives / (False Positives + True Negatives)\")\nprint(\"A perfect classifier would have a point in the top left corner (TPR=1, FPR=0).\")\nprint(\"The diagonal line represents a random classifier.\")\nprint(f\"The closer the curve is to the top left, the better the model's performance. The AUC (Area Under the Curve) summarizes this into a single number. An AUC of 1.0 is perfect, 0.5 is random.\")\n\n\n# 2. Confusion Matrix\npredictions = (dfs['pred'] >= 0.5).astype(int)  # Assuming binary classification.  Threshold *matters*\ntrue_labels = (dfs['target'] == mel_idx).astype(int)\ncm = confusion_matrix(true_labels, predictions)\n\nplt.figure(figsize=(6, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title(f'Confusion Matrix ({run_id})\\nThreshold = 0.5')\nplt.savefig(os.path.join(results_dir, f\"{run_id}_confusion_matrix.png\")) # Save the plot\nplt.show()\n\nprint(\"\\n--- Confusion Matrix Explanation ---\")\nprint(\"The confusion matrix shows the counts of:\")\nprint(\"  - True Positives (TP): Correctly predicted positive cases.\")\nprint(\"  - True Negatives (TN): Correctly predicted negative cases.\")\nprint(\"  - False Positives (FP): Incorrectly predicted positive cases (Type I error).\")\nprint(\"  - False Negatives (FN): Incorrectly predicted negative cases (Type II error).\")\nprint(\"This matrix helps understand the types of errors your model is making.\")\n\n\n# 3. Prediction Distribution (Histogram)\nplt.figure(figsize=(8, 6))\nplt.hist(dfs['pred'], bins=30, color='skyblue', edgecolor='black')\nplt.xlabel('Predicted Probability of Melanoma')\nplt.ylabel('Frequency')\nplt.title(f'Prediction Distribution ({run_id})')\nplt.savefig(os.path.join(results_dir, f\"{run_id}_prediction_distribution.png\"))  # Save\nplt.show()\n\nprint(\"\\n--- Prediction Distribution Explanation ---\")\nprint(\"This histogram shows the distribution of predicted probabilities for the positive class (melanoma).\")\nprint(\"Ideally, you want to see a clear separation between the probabilities for positive and negative cases.\")\nprint(\"A well-calibrated model will have probabilities near 0 for negative cases and near 1 for positive cases.\")\nprint(\"If the distribution is skewed or clustered around 0.5, it suggests the model is uncertain or poorly calibrated.\")\n\n\n\n# 4. ROC AUC per fold\nfold_aucs = []\nfor i in range(5):\n    fold_data = dfs[dfs['fold'] == i]\n    fold_roc_auc = roc_auc_score(fold_data['target'] == mel_idx, fold_data['pred'])\n    fold_aucs.append(fold_roc_auc)\n    print(f'Fold {i} ROC AUC: {fold_roc_auc:.4f}')\n\nplt.figure(figsize=(8, 6))\nplt.bar(range(5), fold_aucs, color='lightgreen')\nplt.xlabel('Fold')\nplt.ylabel('ROC AUC')\nplt.title(f'ROC AUC per Fold ({run_id})')\nplt.ylim([0, 1])  # Set y-axis limit\nplt.savefig(os.path.join(results_dir, f\"{run_id}_roc_auc_per_fold.png\"))  # Save\nplt.show()\n\nprint(\"\\n--- ROC AUC per Fold Explanation ---\")\nprint(\"This bar plot shows the ROC AUC for each fold of your cross-validation.\")\nprint(\"It helps assess the consistency of your model's performance across different subsets of the data.\")\nprint(\"Large variations in AUC between folds might indicate overfitting to specific folds or data imbalances.\")\n\n\n# 5. ROC AUC 2020\nroc_auc_2020 = roc_auc_score(dfs[dfs['is_ext'] == 0]['target'] == mel_idx, dfs[dfs['is_ext'] == 0]['pred'])\nprint(f'Raw auc_2020: {roc_auc_2020:.4f}')\n\n# 6. Ranked Predictions (and AUC)\ndfs2 = dfs.copy()\nfor i in range(5):\n    dfs2.loc[dfs2['fold']==i, 'pred'] = dfs2.loc[dfs2['fold']==i, 'pred'].rank(pct=True)\nranked_roc_auc = roc_auc_score(dfs2['target'] == mel_idx, dfs2['pred'])\nprint(f'Rank per fold auc_all: {ranked_roc_auc:.4f}')\n\ndfs2 = dfs[dfs.is_ext == 0].copy()\nfor i in range(5):\n    dfs2.loc[dfs2['fold'] == i, 'pred'] = dfs2.loc[dfs2['fold'] == i, 'pred'].rank(pct=True)\nranked_roc_auc_2020 = roc_auc_score(dfs2['target'] == mel_idx, dfs2['pred'])\nprint(f'Rank per fold auc_2020: {ranked_roc_auc_2020:.4f}')\n\nprint(\"\\n--- Ranked Predictions Explanation ---\")\nprint(\"Ranking predictions within each fold transforms the raw probabilities into ranks.\")\nprint(\"This makes the ROC AUC metric less sensitive to the absolute values of the probabilities and more focused on the relative ordering.\")\nprint(\"It's useful when the model's calibration is not perfect, but the relative ranking of predictions is still informative.\")\n\n\n# 7. Calibration Plot\nprob_true, prob_pred = calibration_curve(true_labels, dfs['pred'], n_bins=10, strategy='uniform')\n\nplt.figure(figsize=(8, 6))\nplt.plot(prob_pred, prob_true, marker='o', label='Model Calibration')\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('Fraction of Positives')\nplt.title(f'Calibration Plot ({run_id})')\nplt.legend()\nplt.savefig(os.path.join(results_dir, f\"{run_id}_calibration_plot.png\")) # Save\nplt.show()\n\nprint(\"\\n--- Calibration Plot Explanation ---\")\nprint(\"A calibration plot assesses how well the predicted probabilities match the observed frequencies of positive cases.\")\nprint(\"A perfectly calibrated model would have points lying on the diagonal.\")\nprint(\"Points above the diagonal indicate the model is under-confident (predicting probabilities too low).\")\nprint(\"Points below the diagonal indicate the model is over-confident (predicting probabilities too high).\")\n\n\n# --- Calculate Additional Metrics ---\nprecision = precision_score(true_labels, predictions)\nrecall = recall_score(true_labels, predictions)\nf1 = f1_score(true_labels, predictions)\naccuracy = accuracy_score(true_labels, predictions)\n\nprint(f\"\\n--- Additional Metrics (Threshold = 0.5) ---\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-score: {f1:.4f}\")\nprint(f\"Accuracy: {accuracy:.4f}\")\n\n# --- Save Results to CSV ---\n\nresults_df = pd.DataFrame({\n    'run_id': [run_id],\n    'model_type': [enet_type],\n    'use_metadata': [use_meta],\n    'roc_auc_all': [roc_auc],\n    'roc_auc_2020': [roc_auc_2020],\n    'ranked_roc_auc_all': [ranked_roc_auc],\n    'ranked_roc_auc_2020': [ranked_roc_auc_2020],\n    'precision': [precision],\n    'recall': [recall],\n    'f1_score': [f1],\n    'accuracy': [accuracy],\n    'fold_0_auc': [fold_aucs[0] if len(fold_aucs) > 0 else None],\n    'fold_1_auc': [fold_aucs[1] if len(fold_aucs) > 1 else None],\n    'fold_2_auc': [fold_aucs[2] if len(fold_aucs) > 2 else None],\n    'fold_3_auc': [fold_aucs[3] if len(fold_aucs) > 3 else None],\n    'fold_4_auc': [fold_aucs[4] if len(fold_aucs) > 4 else None],\n})\n\n# Save the overall metrics\nresults_df.to_csv(results_filename, index=False)\nprint(f\"Evaluation results saved to: {results_filename}\")\n\n# Save the per-example predictions and true labels (for later analysis/ensembling)\npredictions_df = pd.DataFrame({\n    'image_name': dfs['image_name'],  # Assuming you have an 'image_name' column\n    'true_label': true_labels,\n    'predicted_probability': dfs['pred'],\n    'predicted_label': predictions,\n    'fold': dfs['fold'],\n    'is_ext': dfs['is_ext']\n})\npredictions_df.to_csv(predictions_filename, index=False)\nprint(f\"Predictions saved to: {predictions_filename}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Predict","metadata":{}},{"cell_type":"code","source":"n_test = 8\ndf_test = df_test if not DEBUG else df_test.head(batch_size * 2)\ndataset_test = SIIMISICDataset(df_test, 'test', 'test', transform=transforms_val)\ntest_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, num_workers=num_workers)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"models = []\nfor i_fold in range(5):\n    model = enetv2(enet_type, n_meta_features=0, out_dim=out_dim)\n    model = model.to(device)\n    model_file = os.path.join(model_dir, f'{kernel_type}_best_fold{i_fold}.pth')\n    state_dict = torch.load(model_file)\n    state_dict = {k.replace('module.', ''): state_dict[k] for k in state_dict.keys()}\n    model.load_state_dict(state_dict, strict=True)\n    model.eval()\n    models.append(model)\nlen(models)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"OUTPUTS = []\nPROBS = []\n\nwith torch.no_grad():\n    for (data) in tqdm(test_loader):\n\n        if use_meta:\n            data, meta = data\n            data, meta = data.to(device), meta.to(device)\n            probs = torch.zeros((data.shape[0], out_dim)).to(device)\n            for I in range(n_test):\n                l = model(get_trans(data, I), meta)\n                probs += l.softmax(1)\n        else:\n            data = data.to(device)\n            probs = torch.zeros((data.shape[0], out_dim)).to(device)\n            for model in models:\n                for I in range(n_test):\n                    l = model(get_trans(data, I))\n                    probs += l.softmax(1)\n\n        probs /= n_test * len(models)\n        PROBS.append(probs.detach().cpu())\n\nPROBS = torch.cat(PROBS).numpy()\nOUTPUTS = PROBS[:, mel_idx]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test['target'] = OUTPUTS\ndf_test[['image_name', 'target']].to_csv(f'submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test[['image_name', 'target']].head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Infer Single Image","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# import torch\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# from sklearn.metrics import (\n#     roc_curve, precision_recall_curve, average_precision_score,\n#     confusion_matrix, classification_report, f1_score, roc_auc_score\n# )\n# from typing import Tuple, List, Optional\n\n# class ModelEvaluator:\n#     def __init__(self, model, device, out_dim, mel_idx):\n#         self.model = model\n#         self.device = device\n#         self.out_dim = out_dim\n#         self.mel_idx = mel_idx\n#         self.metrics_history = {\n#             'fold_metrics': [],\n#             'overall_metrics': None\n#         }\n        \n#         # Set style for better-looking plots\n#         plt.style.use('seaborn')\n#         sns.set_palette(\"husl\")\n    \n#     def _calculate_metrics(self, y_true, y_pred, y_pred_proba) -> dict:\n#         \"\"\"\n#         Calculates comprehensive classification metrics and ensures integer counts\n#         for confusion matrix values\n#         \"\"\"\n#         # Calculate confusion matrix and convert to integers\n#         tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n#         tn, fp, fn, tp = int(tn), int(fp), int(fn), int(tp)\n        \n#         return {\n#             'accuracy': (tp + tn) / (tp + tn + fp + fn),\n#             'sensitivity': tp / (tp + fn) if (tp + fn) > 0 else 0,\n#             'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,\n#             'precision': tp / (tp + fp) if (tp + fp) > 0 else 0,\n#             'npv': tn / (tn + fn) if (tn + fn) > 0 else 0,\n#             'f1': f1_score(y_true, y_pred),\n#             'auc': roc_auc_score(y_true, y_pred_proba),\n#             'avg_precision': average_precision_score(y_true, y_pred_proba),\n#             'tp': tp,  # Now guaranteed to be integer\n#             'tn': tn,  # Now guaranteed to be integer\n#             'fp': fp,  # Now guaranteed to be integer\n#             'fn': fn   # Now guaranteed to be integer\n#         }\n    \n#     def plot_confusion_matrices(self, save_path: Optional[str] = None):\n#         \"\"\"\n#         Creates a grid of confusion matrices for each fold with improved formatting\n#         and error handling\n#         \"\"\"\n#         fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n#         axes = axes.ravel()\n        \n#         # Define a custom annotation function\n#         def annotate_heatmap(data, ax):\n#             for i in range(data.shape[0]):\n#                 for j in range(data.shape[1]):\n#                     # Convert to integer explicitly\n#                     value = int(data[i, j])\n#                     text_color = \"white\" if data[i, j] > data.max() / 2 else \"black\"\n#                     ax.text(j + 0.5, i + 0.5, f'{value:,}',\n#                            ha='center', va='center',\n#                            color=text_color, fontsize=10)\n        \n#         # Plot individual fold matrices\n#         for idx, metrics in enumerate(self.metrics_history['fold_metrics']):\n#             # Create confusion matrix for this fold\n#             cm = np.array([\n#                 [metrics['tn'], metrics['fp']],\n#                 [metrics['fn'], metrics['tp']]\n#             ], dtype=np.int64)  # Ensure integer type\n            \n#             # Create heatmap without annotations\n#             sns.heatmap(cm, cmap='Blues', ax=axes[idx],\n#                        xticklabels=['Negative', 'Positive'],\n#                        yticklabels=['Negative', 'Positive'],\n#                        annot=False, cbar=False)\n            \n#             # Add annotations manually\n#             annotate_heatmap(cm, axes[idx])\n            \n#             # Customize appearance\n#             axes[idx].set_title(f'Fold {idx}')\n#             axes[idx].set_xlabel('Predicted')\n#             axes[idx].set_ylabel('Actual')\n        \n#         # Calculate and plot overall confusion matrix\n#         total_cm = np.zeros((2, 2), dtype=np.int64)  # Ensure integer type\n#         for metrics in self.metrics_history['fold_metrics']:\n#             fold_cm = np.array([\n#                 [metrics['tn'], metrics['fp']],\n#                 [metrics['fn'], metrics['tp']]\n#             ], dtype=np.int64)\n#             total_cm += fold_cm\n        \n#         # Create overall heatmap without annotations\n#         sns.heatmap(total_cm, cmap='Blues', ax=axes[-1],\n#                    xticklabels=['Negative', 'Positive'],\n#                    yticklabels=['Negative', 'Positive'],\n#                    annot=False, cbar=True)\n        \n#         # Add annotations manually\n#         annotate_heatmap(total_cm, axes[-1])\n        \n#         # Customize overall matrix appearance\n#         axes[-1].set_title('Overall')\n#         axes[-1].set_xlabel('Predicted')\n#         axes[-1].set_ylabel('Actual')\n        \n#         # Adjust layout\n#         plt.tight_layout()\n        \n#         # Save if path provided\n#         if save_path:\n#             plt.savefig(save_path, bbox_inches='tight', dpi=300)\n            \n#         plt.show()\n#         plt.close()\n    \n#     def plot_prediction_distribution(self, dfs: pd.DataFrame, save_path: Optional[str] = None):\n#         \"\"\"\n#         Plots the distribution of model predictions for positive and negative cases\n#         \"\"\"\n#         plt.figure(figsize=(10, 6))\n        \n#         # Create separate plots for positive and negative cases\n#         sns.kdeplot(data=dfs[dfs['target'] == self.mel_idx]['pred'],\n#                    label='Positive Cases', color='red', alpha=0.6)\n#         sns.kdeplot(data=dfs[dfs['target'] != self.mel_idx]['pred'],\n#                    label='Negative Cases', color='blue', alpha=0.6)\n        \n#         plt.xlabel('Model Prediction Score')\n#         plt.ylabel('Density')\n#         plt.title('Distribution of Model Predictions')\n#         plt.legend()\n        \n#         if save_path:\n#             plt.savefig(save_path, bbox_inches='tight', dpi=300)\n#         plt.show()\n#         plt.close()\n    \n#     def generate_metrics_summary(self) -> pd.DataFrame:\n#         \"\"\"\n#         Generates a comprehensive summary of model performance metrics\n#         \"\"\"\n#         metrics_df = pd.DataFrame(self.metrics_history['fold_metrics'])\n        \n#         # Calculate mean and std for each metric\n#         summary = pd.DataFrame({\n#             'Metric': [],\n#             'Mean': [],\n#             'Std': [],\n#             'Value Range': []\n#         })\n        \n#         for column in metrics_df.columns:\n#             if column not in ['fold']:\n#                 mean_val = metrics_df[column].mean()\n#                 std_val = metrics_df[column].std()\n#                 min_val = metrics_df[column].min()\n#                 max_val = metrics_df[column].max()\n                \n#                 summary = pd.concat([summary, pd.DataFrame({\n#                     'Metric': [column],\n#                     'Mean': [f\"{mean_val:.3f}\"],\n#                     'Std': [f\"{std_val:.3f}\"],\n#                     'Value Range': [f\"{min_val:.3f} - {max_val:.3f}\"]\n#                 })], ignore_index=True)\n        \n#         return summary\n\n# # Example usage remains the same as before\n# # Example usage in your existing code:\n# evaluator = ModelEvaluator(model, device, out_dim, mel_idx)\n\n# PROBS = []\n# dfs = []\n# for fold in range(5):\n#     i_fold = fold\n#     df_valid = df_train[df_train['fold'] == i_fold]\n#     if DEBUG:\n#         df_valid = pd.concat([\n#             df_valid[df_valid['target'] == mel_idx].sample(10),\n#             df_valid[df_valid['target'] != mel_idx].sample(10)\n#         ])\n    \n#     dataset_valid = SIIMISICDataset(df_valid, 'train', 'val', transform=transforms_val)\n#     valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, \n#                                              num_workers=num_workers)\n    \n#     # Load and prepare model\n#     model = enetv2(enet_type, n_meta_features=0, out_dim=out_dim)\n#     model = model.to(device)\n#     model_file = os.path.join(model_dir, f'{kernel_type}_best_fold{i_fold}.pth')\n#     state_dict = torch.load(model_file)\n#     state_dict = {k.replace('module.', ''): state_dict[k] for k in state_dict.keys()}\n#     model.load_state_dict(state_dict, strict=True)\n#     model.eval()\n    \n#     # Evaluate fold\n#     metrics = evaluator.evaluate_fold(valid_loader, i_fold, df_valid, n_test=8)\n#     print(f\"\\nFold {i_fold} Metrics:\")\n#     for metric, value in metrics.items():\n#         if metric != 'fold':\n#             print(f\"{metric}: {value:.3f}\")\n    \n#     # Store predictions for overall analysis\n#     this_LOGITS, this_PROBS = val_epoch(model, valid_loader, \n#                                       is_ext=df_valid['is_ext'].values,\n#                                       n_test=8, get_output=True)\n#     PROBS.append(this_PROBS)\n#     dfs.append(df_valid)\n\n# # Combine results\n# dfs = pd.concat(dfs).reset_index(drop=True)\n# dfs['pred'] = np.concatenate(PROBS).squeeze()[:, mel_idx]\n\n# # Generate visualizations\n# evaluator.plot_roc_curves(dfs, save_path='roc_curves.png')\n# evaluator.plot_confusion_matrices(save_path='confusion_matrices.png')\n# evaluator.plot_prediction_distribution(dfs, save_path='pred_distribution.png')\n\n# # Generate final metrics summary\n# summary_df = evaluator.generate_metrics_summary()\n# print(\"\\nOverall Performance Summary:\")\n# print(summary_df.to_string(index=False))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#patient test images processing\n# patient 1 image name\nin1 = 'hello'\n# patient 1 patient_id\npi1 = 'IP23232'\n# patient 1 sex\ns1 = 'male'\n# patient 1 age\na1 = 70.0\n# patient 1 anatom_site_general_challenge. where its located\nasgc1 = 'torso'\n# patient 1 image filepath\nf1 = '../input/jpeg-melanoma-768x768/test/ISIC_0052060.jpg'\nimport csv\nwith open('datasettesting.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow([\"image_name\", \"patient_id\", \"sex\", \"age_approx\", \"anatom_site_general_challenge\", \"width\", \"height\", \"filepath\"])\n    writer.writerow([in1,pi1, s1, a1, asgc1, 6000, 4000, f1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_single_image = pd.read_csv('datasettesting.csv')\ndf_single_image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# as we only read a single image, so we don't need a dataloader\ndataset_test = SIIMISICDataset(df_single_image, 'test', 'test', transform=transforms_val)\nimage = dataset_test[0]  \nimage = image.to(device).unsqueeze(0)  # a single image need to be added a new axis to act like batch_size = 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n    probs = torch.zeros((image.shape[0], out_dim)).to(device)\n    for model in models:\n        for I in range(n_test):\n            l = model(get_trans(image, I))\n            probs += l.softmax(1)\nprobs /= len(models) * n_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prediction = probs[:, mel_idx].item()\nprediction","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}